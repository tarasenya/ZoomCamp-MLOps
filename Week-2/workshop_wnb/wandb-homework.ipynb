{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad71015",
   "metadata": {},
   "source": [
    "**Q1. Install the Package**\n",
    "\n",
    "To get started with Weights & Biases you'll need to install the appropriate Python package.\n",
    "\n",
    "For this we recommend creating a separate Python environment, for example, you can use conda environments, and then install the package there with pip or conda.\n",
    "\n",
    "Once you installed the package, run the command wandb --version and check the output.\n",
    "\n",
    "What's the version that you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bb2f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of W&B - 0.15.3\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "print(f'Version of W&B - {wandb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f4212",
   "metadata": {},
   "source": [
    "**Q2. Download and preprocess the data**\n",
    "\n",
    "We'll use the Green Taxi Trip Records dataset to predict the amount of tips for each trip.\n",
    "\n",
    "Download the data for January, February and March 2022 in parquet format from here.\n",
    "\n",
    "Use the script preprocess_data.py located in the folder homework-wandb to preprocess the data.\n",
    "\n",
    "The script will:\n",
    "\n",
    "- initialize a Weights & Biases run.\n",
    "- load the data from the folder <TAXI_DATA_FOLDER> (the folder where you have downloaded the data),\n",
    "- fit a DictVectorizer on the training set (January 2022 data),\n",
    "- save the preprocessed datasets and the DictVectorizer to your Weights & Biases dashboard as an artifact of type preprocessed_dataset.\n",
    "Your task is to download the datasets and then execute this command:\n",
    "\n",
    "```bash\n",
    "python preprocess_data.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --raw_data_path <TAXI_DATA_FOLDER> \\\n",
    "  --dest_path ./output\n",
    "```\n",
    "  \n",
    "_Tip_: go to 02-experiment-tracking/homework-wandb/ folder before executing the command and change the value of <WANDB_PROJECT_NAME> to the name of your Weights & Biases project, <WANDB_USERNAME> to your Weights & Biases username, and <TAXI_DATA_FOLDER> to the location where you saved the data.\n",
    "\n",
    "Once you navigate to the Files tab of your artifact on your Weights & Biases page, what's the size of the saved DictVectorizer file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3912f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = r'~/ZoomCamp-MLOps/Week-2/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87bc92",
   "metadata": {},
   "source": [
    "![title](screenshots/run_wandb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa19d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size equals 150.05859375 Kb\n"
     ]
    }
   ],
   "source": [
    "DICT_VECTORIZER_PATH = os.path.join(os.getcwd(),'output/dv.pkl')\n",
    "print(f\"Size equals {os.path.getsize(DICT_VECTORIZER_PATH)/1024} Kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23437937",
   "metadata": {},
   "source": [
    "**Q3. Train a model with Weights & Biases logging**\n",
    "\n",
    "We will train a RandomForestRegressor (from Scikit-Learn) on the taxi dataset.\n",
    "\n",
    "We have prepared the training script _train.py_ for this exercise, which can be also found in the folder homework-wandb.\n",
    "\n",
    "The script will:\n",
    "\n",
    "    *initialize a Weights & Biases run.\n",
    "    *load the preprocessed datasets by fetching them from the Weights & Biases artifact previously created,\n",
    "    *train the model on the training set,\n",
    "    *calculate the MSE score on the validation set and log it to Weights & Biases,\n",
    "    *save the trained model and log it to Weights & Biases as a model artifact.\n",
    "\n",
    "Your task is to modify the script to enable to add Weights & Biases logging, execute the script and then check the Weights & Biases run UI to check that the experiment run was properly tracked.\n",
    "\n",
    "TODO 1: log mse to Weights & Biases under the key \"MSE\"\n",
    "\n",
    "TODO 2: log regressor.pkl as an artifact of type model, refer to the official docs in order to know more about logging artifacts.\n",
    "\n",
    "You can run the script using:\n",
    "\n",
    "```bash\n",
    "python train.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_USERNAME>/<WANDB_PROJECT_NAME>/NYC-Taxi:v0\"\n",
    "``` \n",
    "\n",
    "Tip 1: You can find the artifact address under the Usage tab in the respective artifact's page.\n",
    "\n",
    "Tip 2: don't modify the hyperparameters of the model to make sure that the training will finish quickly.\n",
    "\n",
    "Once you have successfully ran the script, navigate the Overview section of the run in the Weights & Biases UI and scroll down to the Configs. What is the value of the max_depth parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --wandb_project GreenTaxi --wandb_entity tarasenya --data_artifact \"//NYC-Taxi:v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2d2ad",
   "metadata": {},
   "source": [
    "**Q4. Tune model hyperparameters**\n",
    "\n",
    "Now let's try to reduce the validation error by tuning the hyperparameters of the RandomForestRegressor using Weights & Biases Sweeps. We have prepared the script sweep.py for this exercise in the homework-wandb directory.\n",
    "\n",
    "Your task is to modify sweep.py to pass the parameters n_estimators, min_samples_split and min_samples_leaf from config to RandomForestRegressor inside the run_train() function. Then we will run the sweep to figure out not only the best best of hyperparameters for training our model, but also to analyze the most optimum trends in different hyperparameters. We can run the sweep using:\n",
    "```bash\n",
    "python sweep.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_USERNAME>/<WANDB_PROJECT_NAME>/NYC-Taxi:v0\"\n",
    "```  \n",
    "\n",
    "This command will run the sweep for 5 iterations using the Bayesian Optimization and HyperBand method proposed by the paper BOHB: Robust and Efficient Hyperparameter Optimization at Scale. You can take a look at the sweep on your Weights & Biases dashboard, take a look at the Parameter Inportance Panel and the Parallel Coordinates Plot to determine, and analyze which hyperparameter is the most important:\n",
    "\n",
    "   * max_depth\n",
    "   * n_estimators\n",
    "   * min_samples_split <--this\n",
    "   * min_samples_leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "python sweep.py --wandb_project GreenTaxi --wandb_entity tarasenya --data_artifact \"tarasenya/GreenTaxi/NYC-Taxi:v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bb019",
   "metadata": {},
   "source": [
    "![title](screenshots/sweep_importance_of_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcaf6e7",
   "metadata": {},
   "source": [
    "**Q5. Link the best model to the model registry**\n",
    "\n",
    "Now that we have obtained the optimal set of hyperparameters and trained the best model, we can assume that we are ready to test some of these models in production. In this exercise, you'll create a model registry and link the best model from the Sweep to the model registry.\n",
    "\n",
    "First, you will need to create a Registered Model to hold all the candidate models for your particular modeling task. You can refer to this section of the official docs to learn how to create a registered model using the Weights & Biases UI.\n",
    "\n",
    "Once you have created the Registered Model successfully, you can navigate to the best run of your sweep, navigate to the model artifact created by the particular run, and click on the Link to Registry option from the UI. This would link the model artifact to the Registered Model. You can choose to add some suitable aliases for the Registered Model, such as production, best, etc.\n",
    "\n",
    "Now that the model artifact is linked to the Registered Model, which of these information do we see on the Registered Model UI?\n",
    "\n",
    "    * Versioning\n",
    "    * Metadata\n",
    "    * Aliases\n",
    "    * Metric (MSE)\n",
    "    * Source run\n",
    "    * All of these <--this\n",
    "    * None of these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07c1d6",
   "metadata": {},
   "source": [
    "![title](screenshots/model_registration.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZoomCamp-MLOps",
   "language": "python",
   "name": "zoomcamp-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
